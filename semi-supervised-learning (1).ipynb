{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Semi-Supervised Learning Process\n",
      "Initial labeled samples: 416\n",
      "Validation samples: 74\n",
      "Unlabeled samples: 103325\n",
      "Number of unique categories: 49\n",
      "\n",
      "\n",
      "Iteration 1: Pseudo-Labeling\n",
      "Current confidence threshold: 0.850\n",
      "Remaining unlabeled samples: 103325\n",
      "Current training set size: 416\n",
      "Validation accuracy: 0.2297\n",
      "Found 0 high-confidence samples\n",
      "Adjusting model parameters:\n",
      "n_estimators: 200 -> 400\n",
      "max_depth: 10 -> 14\n",
      "min_samples_split: 10 -> 6\n",
      "min_samples_leaf: 4 -> 2\n",
      "New validation accuracy: 0.2838\n",
      "Improved performance with new parameters!\n",
      "New best model saved!\n",
      "\n",
      "Iteration 2: Pseudo-Labeling\n",
      "Current confidence threshold: 0.830\n",
      "Remaining unlabeled samples: 103325\n",
      "Current training set size: 416\n",
      "Validation accuracy: 0.2838\n",
      "Found 0 high-confidence samples\n",
      "Adjusting model parameters:\n",
      "n_estimators: 400 -> 600\n",
      "max_depth: 14 -> 18\n",
      "min_samples_split: 6 -> 2\n",
      "min_samples_leaf: 2 -> 1\n",
      "New validation accuracy: 0.2973\n",
      "Improved performance with new parameters!\n",
      "New best model saved!\n",
      "\n",
      "Iteration 3: Pseudo-Labeling\n",
      "Current confidence threshold: 0.810\n",
      "Remaining unlabeled samples: 103325\n",
      "Current training set size: 416\n",
      "Validation accuracy: 0.2973\n",
      "Found 0 high-confidence samples\n",
      "Adjusting model parameters:\n",
      "n_estimators: 600 -> 800\n",
      "max_depth: 18 -> 22\n",
      "New validation accuracy: 0.2568\n",
      "No improvement with parameter tuning\n",
      "\n",
      "Iteration 4: Pseudo-Labeling\n",
      "Current confidence threshold: 0.790\n",
      "Remaining unlabeled samples: 103325\n",
      "Current training set size: 416\n",
      "Validation accuracy: 0.2568\n",
      "Found 0 high-confidence samples\n",
      "Adjusting model parameters:\n",
      "n_estimators: 800 -> 1000\n",
      "max_depth: 22 -> 26\n",
      "New validation accuracy: 0.2973\n",
      "Improved performance with new parameters!\n",
      "\n",
      "Iteration 5: Pseudo-Labeling\n",
      "Current confidence threshold: 0.770\n",
      "Remaining unlabeled samples: 103325\n",
      "Current training set size: 416\n",
      "Validation accuracy: 0.2973\n",
      "Found 0 high-confidence samples\n",
      "Adjusting model parameters:\n",
      "max_depth: 26 -> 30\n",
      "New validation accuracy: 0.3108\n",
      "Improved performance with new parameters!\n",
      "New best model saved!\n",
      "\n",
      "Iteration 6: Pseudo-Labeling\n",
      "Current confidence threshold: 0.750\n",
      "Remaining unlabeled samples: 103325\n",
      "Current training set size: 416\n",
      "Validation accuracy: 0.3108\n",
      "Found 0 high-confidence samples\n",
      "No high-confidence pseudo-labels found in this iteration.\n",
      "\n",
      "Iteration 7: Pseudo-Labeling\n",
      "Current confidence threshold: 0.730\n",
      "Remaining unlabeled samples: 103325\n",
      "Current training set size: 416\n",
      "Validation accuracy: 0.3108\n",
      "Found 0 high-confidence samples\n",
      "No high-confidence pseudo-labels found in this iteration.\n",
      "\n",
      "Iteration 8: Pseudo-Labeling\n",
      "Current confidence threshold: 0.710\n",
      "Remaining unlabeled samples: 103325\n",
      "Current training set size: 416\n",
      "Validation accuracy: 0.3108\n",
      "Found 0 high-confidence samples\n",
      "No high-confidence pseudo-labels found in this iteration.\n",
      "\n",
      "Iteration 9: Pseudo-Labeling\n",
      "Current confidence threshold: 0.690\n",
      "Remaining unlabeled samples: 103325\n",
      "Current training set size: 416\n",
      "Validation accuracy: 0.3108\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load datasets\n",
    "labeled_data = pd.read_csv(\"sampled_dataset.csv\")  # Small labeled dataset\n",
    "unlabeled_data = pd.read_csv(\"remaining_dataset.csv\")  # Large unlabelled dataset\n",
    "\n",
    "# Feature Extraction (Example: TF-IDF for text data)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_labeled = vectorizer.fit_transform(labeled_data[\"crimeaditionalinfo\"]).toarray()\n",
    "X_unlabeled = vectorizer.transform(unlabeled_data[\"crimeaditionalinfo\"]).toarray()\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_labeled = scaler.fit_transform(X_labeled)\n",
    "X_unlabeled = scaler.transform(X_unlabeled)\n",
    "\n",
    "# Encode string labels to integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_labeled = label_encoder.fit_transform(labeled_data[\"final_subcategory\"].values)\n",
    "\n",
    "# Adjust thresholds for better sample selection\n",
    "confidence_threshold = 0.85  # Increased for higher quality initial predictions\n",
    "min_confidence_threshold = 0.75  # Increased minimum threshold\n",
    "distribution_similarity_threshold = 0.2  # Adjusted for better balance\n",
    "cluster_purity_threshold = 0.75\n",
    "max_iterations = 50  # Reduced to prevent overfitting\n",
    "validation_size = 0.15  # Reduced to have more training data\n",
    "n_clusters = len(np.unique(y_labeled))\n",
    "\n",
    "def tune_model_parameters(val_score, current_params):\n",
    "    \"\"\"Unified parameter tuning based on validation performance and current parameters\"\"\"\n",
    "    new_params = current_params.copy()\n",
    "    \n",
    "    # More aggressive parameter tuning for very low performance\n",
    "    if val_score < 0.3:\n",
    "        new_params['n_estimators'] = min(current_params['n_estimators'] + 200, 1000)  # Increased max and step\n",
    "        new_params['max_depth'] = min(current_params['max_depth'] + 4, 30)  # Increased max and step\n",
    "        new_params['min_samples_split'] = max(current_params['min_samples_split'] - 4, 2)\n",
    "        new_params['min_samples_leaf'] = max(current_params['min_samples_leaf'] - 2, 1)\n",
    "    elif val_score < 0.5:\n",
    "        new_params['n_estimators'] = min(current_params['n_estimators'] + 100, 1000)\n",
    "        new_params['max_depth'] = min(current_params['max_depth'] + 2, 30)\n",
    "        new_params['min_samples_split'] = max(current_params['min_samples_split'] - 2, 2)\n",
    "        new_params['min_samples_leaf'] = max(current_params['min_samples_leaf'] - 1, 1)\n",
    "    \n",
    "    return new_params\n",
    "\n",
    "# Create classifier with specific parameters\n",
    "def create_classifier(params):\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        min_samples_leaf=params['min_samples_leaf'],\n",
    "        max_features='sqrt',\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced_subsample'\n",
    "    )\n",
    "\n",
    "# Modified dynamic confidence threshold adjustment\n",
    "def adjust_confidence_threshold(iteration, base_threshold=0.85, min_threshold=0.75):\n",
    "    # More aggressive threshold reduction\n",
    "    decay = iteration * 0.02  # Doubled decay rate\n",
    "    if iteration > 10:  # After 10 iterations, decay even faster\n",
    "        decay = 0.2 + (iteration - 10) * 0.03\n",
    "    return max(base_threshold - decay, min_threshold * 0.9)  # Allow going slightly below min_threshold\n",
    "\n",
    "# Initialize parameters\n",
    "current_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 4\n",
    "}\n",
    "\n",
    "# Initialize classifier with starting parameters\n",
    "clf = create_classifier(current_params)\n",
    "\n",
    "# Add class distribution tracking\n",
    "initial_class_distribution = np.bincount(y_labeled) / len(y_labeled)\n",
    "\n",
    "# Initialize tracking metrics\n",
    "best_val_score = 0\n",
    "best_model = None\n",
    "iteration_metrics = []\n",
    "patience = 5  # Number of iterations to wait before early stopping\n",
    "consecutive_no_improvement = 0\n",
    "\n",
    "# Create a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_labeled, y_labeled, test_size=validation_size, random_state=42, stratify=y_labeled\n",
    ")\n",
    "\n",
    "# Keep track of remaining unlabeled data\n",
    "current_X_unlabeled = X_unlabeled.copy()\n",
    "current_unlabeled_data = unlabeled_data.copy()\n",
    "\n",
    "print(\"Starting Semi-Supervised Learning Process\")\n",
    "print(f\"Initial labeled samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Unlabeled samples: {len(X_unlabeled)}\")\n",
    "print(f\"Number of unique categories: {n_clusters}\\n\")\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    # Update confidence threshold dynamically\n",
    "    current_confidence_threshold = adjust_confidence_threshold(iteration, \n",
    "                                                            confidence_threshold, \n",
    "                                                            min_confidence_threshold)\n",
    "    \n",
    "    print(f\"\\nIteration {iteration+1}: Pseudo-Labeling\")\n",
    "    print(f\"Current confidence threshold: {current_confidence_threshold:.3f}\")\n",
    "    print(f\"Remaining unlabeled samples: {len(current_X_unlabeled)}\")\n",
    "    \n",
    "    if len(current_X_unlabeled) == 0:\n",
    "        print(\"All data has been labeled. Stopping the process.\")\n",
    "        break\n",
    "\n",
    "    # Train model on current labeled data\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_score = clf.score(X_val, y_val)\n",
    "    print(f\"Current training set size: {len(X_train)}\")\n",
    "    print(f\"Validation accuracy: {val_score:.4f}\")\n",
    "    \n",
    "    # Predict on current unlabelled data\n",
    "    pseudo_probs = clf.predict_proba(current_X_unlabeled)\n",
    "    pseudo_labels = clf.predict(current_X_unlabeled)\n",
    "    \n",
    "    # More stringent selection of high-confidence predictions\n",
    "    confidence_scores = np.max(pseudo_probs, axis=1)\n",
    "    high_confidence_mask = confidence_scores > current_confidence_threshold\n",
    "    \n",
    "    X_new = current_X_unlabeled[high_confidence_mask]\n",
    "    y_new = pseudo_labels[high_confidence_mask]\n",
    "    \n",
    "    print(f\"Found {np.sum(high_confidence_mask)} high-confidence samples\")\n",
    "    \n",
    "    # Check if we need to tune parameters\n",
    "    should_tune = (val_score < 0.5 or len(X_new) == 0)\n",
    "    \n",
    "    if should_tune:\n",
    "        new_params = tune_model_parameters(val_score, current_params)\n",
    "        \n",
    "        # Only create new classifier if parameters actually changed\n",
    "        if new_params != current_params:\n",
    "            print(\"Adjusting model parameters:\")\n",
    "            for param in new_params:\n",
    "                if new_params[param] != current_params[param]:\n",
    "                    print(f\"{param}: {current_params[param]} -> {new_params[param]}\")\n",
    "            \n",
    "            current_params = new_params\n",
    "            clf = create_classifier(current_params)\n",
    "            \n",
    "            # Retrain with new parameters\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate new model\n",
    "            new_val_score = clf.score(X_val, y_val)\n",
    "            print(f\"New validation accuracy: {new_val_score:.4f}\")\n",
    "            \n",
    "            if new_val_score > val_score:\n",
    "                print(\"Improved performance with new parameters!\")\n",
    "                val_score = new_val_score\n",
    "                if val_score > best_val_score:\n",
    "                    best_val_score = val_score\n",
    "                    best_model = clf\n",
    "                    print(\"New best model saved!\")\n",
    "            else:\n",
    "                print(\"No improvement with parameter tuning\")\n",
    "                consecutive_no_improvement += 1\n",
    "            \n",
    "            continue\n",
    "    \n",
    "    # Modified distribution similarity check\n",
    "    if np.sum(high_confidence_mask) > 0:\n",
    "        potential_new_labels = pseudo_labels[high_confidence_mask]\n",
    "        new_distribution = np.bincount(potential_new_labels, minlength=len(initial_class_distribution))\n",
    "        new_distribution = new_distribution / len(potential_new_labels)\n",
    "        \n",
    "        dist_similarity = np.sum(np.minimum(initial_class_distribution, new_distribution))\n",
    "        \n",
    "        print(f\"Distribution similarity: {dist_similarity:.4f}\")\n",
    "        if dist_similarity < distribution_similarity_threshold:\n",
    "            print(\"Skipping iteration due to significant class distribution shift\")\n",
    "            consecutive_no_improvement += 1\n",
    "            continue\n",
    "    \n",
    "    if len(X_new) == 0:\n",
    "        print(\"No high-confidence pseudo-labels found in this iteration.\")\n",
    "        consecutive_no_improvement += 1\n",
    "        continue\n",
    "        \n",
    "    # Modified clustering validation\n",
    "    if len(X_new) > n_clusters:\n",
    "        validation_clusters = min(n_clusters, 20)  # Increased from 10\n",
    "        kmeans = KMeans(n_clusters=validation_clusters, random_state=42, n_init=10)\n",
    "        \n",
    "        try:\n",
    "            kmeans.fit(X_new)\n",
    "            cluster_labels = kmeans.labels_\n",
    "            \n",
    "            # Check cluster purity\n",
    "            cluster_purity = []\n",
    "            for cluster in range(validation_clusters):\n",
    "                cluster_mask = cluster_labels == cluster\n",
    "                if np.sum(cluster_mask) > 0:\n",
    "                    labels_in_cluster = y_new[cluster_mask]\n",
    "                    majority_label_count = np.max(np.bincount(labels_in_cluster))\n",
    "                    purity = majority_label_count / np.sum(cluster_mask)\n",
    "                    cluster_purity.append(purity)\n",
    "            \n",
    "            avg_purity = np.mean(cluster_purity)\n",
    "            print(f\"Average cluster purity: {avg_purity:.4f}\")\n",
    "            \n",
    "            # More lenient cluster purity check\n",
    "            if avg_purity > cluster_purity_threshold:\n",
    "                X_train = np.vstack([X_train, X_new])\n",
    "                y_train = np.concatenate([y_train, y_new])\n",
    "                print(f\"Added {len(X_new)} validated pseudo-labeled samples\")\n",
    "                consecutive_no_improvement = 0  # Reset counter on successful addition\n",
    "            else:\n",
    "                print(\"Pseudo-labels rejected due to low cluster purity\")\n",
    "                consecutive_no_improvement += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Clustering validation failed: {e}\")\n",
    "            consecutive_no_improvement += 1\n",
    "            continue\n",
    "            \n",
    "        # Store metrics for this iteration\n",
    "        iteration_metrics.append({\n",
    "            'iteration': iteration + 1,\n",
    "            'validation_score': val_score,\n",
    "            'pseudo_labeled_samples': len(X_new),\n",
    "            'total_training_samples': len(X_train),\n",
    "            'cluster_purity': avg_purity\n",
    "        })\n",
    "        \n",
    "        # Update remaining unlabeled data\n",
    "        current_X_unlabeled = current_X_unlabeled[~high_confidence_mask]\n",
    "        current_unlabeled_data = current_unlabeled_data[~high_confidence_mask]\n",
    "        print(f\"Remaining unlabeled samples after this iteration: {len(current_X_unlabeled)}\")\n",
    "    else:\n",
    "        print(f\"Not enough samples ({len(X_new)}) for meaningful clustering validation\")\n",
    "        if np.mean(confidence_scores[high_confidence_mask]) > 0.98:  # Very high confidence threshold for small batches\n",
    "            X_train = np.vstack([X_train, X_new])\n",
    "            y_train = np.concatenate([y_train, y_new])\n",
    "            print(f\"Added {len(X_new)} samples from small batch (very high confidence)\")\n",
    "            consecutive_no_improvement = 0  # Reset counter on successful addition\n",
    "        else:\n",
    "            consecutive_no_improvement += 1\n",
    "        \n",
    "        # Update remaining unlabeled data\n",
    "        current_X_unlabeled = current_X_unlabeled[~high_confidence_mask]\n",
    "        current_unlabeled_data = current_unlabeled_data[~high_confidence_mask]\n",
    "    \n",
    "    if len(X_new) == 0 and consecutive_no_improvement >= 3:\n",
    "        # If we're stuck, temporarily lower the confidence threshold significantly\n",
    "        temp_threshold = current_confidence_threshold * 0.8\n",
    "        print(f\"Temporarily lowering confidence threshold to {temp_threshold:.3f}\")\n",
    "        \n",
    "        pseudo_probs = clf.predict_proba(current_X_unlabeled)\n",
    "        pseudo_labels = clf.predict(current_X_unlabeled)\n",
    "        confidence_scores = np.max(pseudo_probs, axis=1)\n",
    "        high_confidence_mask = confidence_scores > temp_threshold\n",
    "        \n",
    "        # Take only the top 1% most confident predictions\n",
    "        if np.sum(high_confidence_mask) > 0:\n",
    "            confidence_threshold_idx = max(int(len(current_X_unlabeled) * 0.01), 1)\n",
    "            top_confidence_indices = np.argsort(confidence_scores)[-confidence_threshold_idx:]\n",
    "            high_confidence_mask = np.zeros_like(high_confidence_mask)\n",
    "            high_confidence_mask[top_confidence_indices] = True\n",
    "            \n",
    "            X_new = current_X_unlabeled[high_confidence_mask]\n",
    "            y_new = pseudo_labels[high_confidence_mask]\n",
    "            print(f\"Added {len(X_new)} samples with lowered threshold\")\n",
    "            \n",
    "            X_train = np.vstack([X_train, X_new])\n",
    "            y_train = np.concatenate([y_train, y_new])\n",
    "            current_X_unlabeled = current_X_unlabeled[~high_confidence_mask]\n",
    "            current_unlabeled_data = current_unlabeled_data[~high_confidence_mask]\n",
    "            consecutive_no_improvement = 0\n",
    "            continue\n",
    "        \n",
    "    # Early stopping check\n",
    "    if consecutive_no_improvement >= patience:\n",
    "        print(f\"\\nStopping early due to {patience} iterations without improvement\")\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_score:.4f}\")\n",
    "\n",
    "# Print detailed metrics for best model\n",
    "print(\"\\nDetailed Classification Report for Best Model:\")\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Use the best model for final predictions\n",
    "final_labels = best_model.predict(X_unlabeled)\n",
    "\n",
    "# Convert numeric labels back to original categories\n",
    "final_labels = label_encoder.inverse_transform(final_labels)\n",
    "\n",
    "# Save the labeled dataset\n",
    "unlabeled_data[\"final_subcategory\"] = final_labels\n",
    "unlabeled_data.to_csv(\"fully_labeled_data.csv\", index=False)\n",
    "print(\"\\nFinal dataset saved!\")\n",
    "\n",
    "# Display iteration metrics\n",
    "print(\"\\nIteration Metrics:\")\n",
    "metrics_df = pd.DataFrame(iteration_metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>crimeaditionalinfo</th>\n",
       "      <th>final_category</th>\n",
       "      <th>final_subcategory</th>\n",
       "      <th>category_justification</th>\n",
       "      <th>subcategory_justification</th>\n",
       "      <th>confidence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Online Cyber Trafficking</td>\n",
       "      <td>Online Trafficking</td>\n",
       "      <td>SIR I HAVE GET SMS WITH PRE APPORVED LOAN  IJU...</td>\n",
       "      <td>online_financial_fraud</td>\n",
       "      <td>fraud_callvishing</td>\n",
       "      <td>The content describes a fraudulent loan scheme...</td>\n",
       "      <td>The user was contacted via WhatsApp and pressu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Online Cyber Trafficking</td>\n",
       "      <td>Online Trafficking</td>\n",
       "      <td>this number frauder call me I had ordered on a...</td>\n",
       "      <td>online_financial_fraud</td>\n",
       "      <td>debitcredit_card_fraudsim_swap_fraud</td>\n",
       "      <td>The content describes a situation where the us...</td>\n",
       "      <td>The user describes a debit card being used fra...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Online Cyber Trafficking</td>\n",
       "      <td>Online Trafficking</td>\n",
       "      <td>I have received a notification by chrome he sa...</td>\n",
       "      <td>online_financial_fraud</td>\n",
       "      <td>debitcredit_card_fraudsim_swap_fraud</td>\n",
       "      <td>The content describes a user losing money afte...</td>\n",
       "      <td>The user's account balance being deducted afte...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>Online Cyber Trafficking</td>\n",
       "      <td>Online Trafficking</td>\n",
       "      <td>The app is in playstore with name of the five ...</td>\n",
       "      <td>online_financial_fraud</td>\n",
       "      <td>attacks_or_incidents_affecting_digital_payment...</td>\n",
       "      <td>The content describes a scenario where the use...</td>\n",
       "      <td>The mention of investing money through an app ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>Online Cyber Trafficking</td>\n",
       "      <td>Online Trafficking</td>\n",
       "      <td>MY TR ID SBIUPI ID mpMTiRBaQPTVUZXBAwlwhDqcsUu...</td>\n",
       "      <td>online_financial_fraud</td>\n",
       "      <td>upi_related_frauds</td>\n",
       "      <td>The content mentions SBIUPI ID and OTP, indica...</td>\n",
       "      <td>The mention of UPI ID suggests the fraud is re...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  category        sub_category  \\\n",
       "0         0.0  Online Cyber Trafficking  Online Trafficking   \n",
       "1         1.0  Online Cyber Trafficking  Online Trafficking   \n",
       "2         3.0  Online Cyber Trafficking  Online Trafficking   \n",
       "3         6.0  Online Cyber Trafficking  Online Trafficking   \n",
       "4        14.0  Online Cyber Trafficking  Online Trafficking   \n",
       "\n",
       "                                  crimeaditionalinfo          final_category  \\\n",
       "0  SIR I HAVE GET SMS WITH PRE APPORVED LOAN  IJU...  online_financial_fraud   \n",
       "1  this number frauder call me I had ordered on a...  online_financial_fraud   \n",
       "2  I have received a notification by chrome he sa...  online_financial_fraud   \n",
       "3  The app is in playstore with name of the five ...  online_financial_fraud   \n",
       "4  MY TR ID SBIUPI ID mpMTiRBaQPTVUZXBAwlwhDqcsUu...  online_financial_fraud   \n",
       "\n",
       "                                   final_subcategory  \\\n",
       "0                                  fraud_callvishing   \n",
       "1               debitcredit_card_fraudsim_swap_fraud   \n",
       "2               debitcredit_card_fraudsim_swap_fraud   \n",
       "3  attacks_or_incidents_affecting_digital_payment...   \n",
       "4                                 upi_related_frauds   \n",
       "\n",
       "                              category_justification  \\\n",
       "0  The content describes a fraudulent loan scheme...   \n",
       "1  The content describes a situation where the us...   \n",
       "2  The content describes a user losing money afte...   \n",
       "3  The content describes a scenario where the use...   \n",
       "4  The content mentions SBIUPI ID and OTP, indica...   \n",
       "\n",
       "                           subcategory_justification  confidence_score  \n",
       "0  The user was contacted via WhatsApp and pressu...               NaN  \n",
       "1  The user describes a debit card being used fra...               NaN  \n",
       "2  The user's account balance being deducted afte...               NaN  \n",
       "3  The mention of investing money through an app ...               NaN  \n",
       "4  The mention of UPI ID suggests the fraud is re...               NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example CSV path â€” replace with your own file path\n",
    "csv_file_path = \"50_subcats_train_test.csv\"\n",
    "\n",
    "# Read the CSV into a pandas DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_subcategory\n",
      "aadhar_enabled_payment_system_aeps_fraud                                        10\n",
      "impersonating_email                                                             10\n",
      "intimidating_email                                                              10\n",
      "malicious_mobile_app_attacks                                                    10\n",
      "malware_attack                                                                  10\n",
      "online_financial_fraud                                                          10\n",
      "online_gambling_betting                                                         10\n",
      "online_job_fraud                                                                10\n",
      "online_matrimonial_fraud                                                        10\n",
      "online_trafficking                                                              10\n",
      "other                                                                           10\n",
      "password_attacks                                                                10\n",
      "profile_hacking_identity_theft                                                  10\n",
      "provocative_speech_for_unlawful_acts                                            10\n",
      "ransomware_attack                                                               10\n",
      "rapegang_rape_rgrsexually_abusive_content                                       10\n",
      "sale_publishing_and_transmitting_obscene_material_sexually_explicit_material    10\n",
      "sexual_harassment                                                               10\n",
      "sexually_explicit_act                                                           10\n",
      "sexually_obscene_material                                                       10\n",
      "unauthorised_accessdata_breach                                                  10\n",
      "unauthorized_social_media_access                                                10\n",
      "upi_related_frauds                                                              10\n",
      "internet_banking_related_fraud                                                  10\n",
      "identity_theft_spoofing_and_phishing_attacks                                    10\n",
      "against_interest_of_sovereignty_or_integrity_of_india                           10\n",
      "fraud_callvishing                                                               10\n",
      "attacks_on_servers_database_mail_dns_and_network_devices_routers                10\n",
      "attacks_or_incidents_affecting_digital_payment_systems                          10\n",
      "business_email_compromiseemail_takeover                                         10\n",
      "cheating_by_impersonation                                                       10\n",
      "child_pornography_cpchild_sexual_abuse_material_csam                            10\n",
      "cryptocurrency_fraud                                                            10\n",
      "cyber_blackmailing_stalking_sexting                                             10\n",
      "cyber_blackmailing_threatening                                                  10\n",
      "cyber_bullying_stalking_sexting                                                 10\n",
      "cyber_espionage                                                                 10\n",
      "damage_to_computer_computer_systems_etc                                         10\n",
      "data_breach_theft                                                               10\n",
      "debitcredit_card_fraudsim_swap_fraud                                            10\n",
      "dematdepository_fraud                                                           10\n",
      "denial_of_service_dos_distributed_denial_of_service_ddos_attacks                10\n",
      "disinformation_or_misinformation_campaigns                                      10\n",
      "email_hacking                                                                   10\n",
      "email_phishing                                                                  10\n",
      "ewallet_related_fraud                                                           10\n",
      "fake_mobile_apps                                                                10\n",
      "fakeimpersonating_profile                                                       10\n",
      "website_defacementhacking                                                       10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Name of the column that contains the labels\n",
    "label_column = \"final_subcategory\"\n",
    "\n",
    "# Convert labels to lowercase before grouping\n",
    "df[label_column] = df[label_column].str.lower()\n",
    "\n",
    "# Group by the label column and sample 10 rows from each label\n",
    "df_sampled = df.groupby(label_column).sample(n=10, random_state=42)\n",
    "\n",
    "# Check the distribution\n",
    "\n",
    "print(df_sampled[label_column].value_counts())\n",
    "\n",
    "# (Optional) Save the sampled DataFrame to a new CSV\n",
    "df_sampled.to_csv(\"sampled_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining = df.drop(df_sampled.index)\n",
    "df_remaining.to_csv(\"remaining_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, 103325)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sampled), len(df_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
