--- START OF FILE encoder_only_transformer_ag_news_classification_(1).py ---

# -*- coding: utf-8 -*-
"""Encoder_only_transformer_AG_News_classification (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DaicwDtacbe16C7uM4YofBkp7OZcdE4L

# Encoder-only transformer model for AG News classification

In this notebook, I train a encoder-only transformer to do text classification on the AG_NEWS dataset.
Text classification seems to be a pretty simple task, and using transformer is probably overkill. But this is my first time implementing the transformer structure from scratch (including the self-attention module), and it was fun :-)
"""

# some commands in th is notebook require torchtext 0.12.0
# !pip install  torchtext --upgrade --quiet
# !pip install torchdata --quiet
# !pip install torchinfo --quiet

import collections
import math
from dataclasses import dataclass
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as functional
import torchtext
import torchdata
from torch.utils.data import DataLoader
from tqdm import tqdm
import torchinfo
import pandas as pd
import os  # Import os for path manipulation

seed = 42
torch.manual_seed(seed)
np.random.seed(seed)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(DEVICE)

"""## Data processing"""

# One can easily modify the data processing part of this code to accommodate for   other datasets for text classification listed in https://pytorch.org/text/stable/datasets.html#text-classification
# from torchtext.datasets import AG_NEWS  # Removed AG_NEWS import

# Load your custom CSV data
train_df = pd.read_csv("train.csv")  # Replace "train.csv" with your train file path
test_df = pd.read_csv("test.csv")    # Replace "test.csv" with your test file path

# Assuming your CSV has columns 'crime info' and 'category'
TEXT_COL = 'crime info'
LABEL_COL = 'category'

tokenizer = torchtext.data.utils.get_tokenizer('basic_english')

# Prepare data iterators from pandas DataFrames
def create_iter(df, text_col, label_col):
    for index, row in df.iterrows():
        yield row[label_col], row[text_col]

train_iter = create_iter(train_df, TEXT_COL, LABEL_COL)
test_iter = create_iter(test_df, TEXT_COL, LABEL_COL)

# Get unique categories and create label mapping
categories = sorted(train_df[LABEL_COL].unique())
num_classes = len(categories)
category_to_id = {category: id_ for id_, category in enumerate(categories)}
id_to_category = {id_: category for id_, category in enumerate(categories)}

# convert the labels to be in range(0, num_classes)
y_train = torch.tensor([category_to_id[label] for (label, text) in train_iter])
y_test  = torch.tensor([category_to_id[label] for (label, text) in test_iter])

train_iter = create_iter(train_df, TEXT_COL, LABEL_COL) # Recreate iterator as it's consumed
test_iter = create_iter(test_df, TEXT_COL, LABEL_COL)   # Recreate iterator as it's consumed


# There are many "\\" in the texts in the AG_news dataset, we get rid of them.
train_iter = ((label, text.replace("\\", " ") if isinstance(text, str) else "") for label, text in train_iter)
test_iter  = ((label, text.replace("\\", " ") if isinstance(text, str) else "") for label, text in test_iter)

# tokenize the texts, and truncate the number of words in each text to max_seq_len
max_seq_len = 100
x_train_texts = [tokenizer(text.lower())[0:max_seq_len]
                 for (label, text) in train_iter]
x_test_texts  = [tokenizer(text.lower())[0:max_seq_len]
                 for (label, text) in test_iter]

# build the vocabulary and word-to-integer map
counter = collections.Counter()
for text in x_train_texts:
    counter.update(text)

vocab_size = 15000
most_common_words = np.array(counter.most_common(vocab_size - 2))
vocab = most_common_words[:,0]

# indexes for the padding token, and unknown tokens
PAD = 0
UNK = 1
word_to_id = {vocab[i]: i + 2 for i in range(len(vocab))}

# map the words in the training and test texts to integers
x_train = [torch.tensor([word_to_id.get(word, UNK) for word in text])
           for text in x_train_texts]
x_test  = [torch.tensor([word_to_id.get(word, UNK) for word in text])
          for text in x_test_texts]
x_test = torch.nn.utils.rnn.pad_sequence(x_test,
                                batch_first=True, padding_value = PAD)

# constructing the dataset in order to be compatible with torch.utils.data.Dataloader
class AGNewsDataset: # Renamed to CustomDataset for clarity, but AGNewsDataset works as well
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, item):
        return self.features[item], self.labels[item]


train_dataset = AGNewsDataset(x_train, y_train)
test_dataset  = AGNewsDataset(x_test, y_test)

# collate_fn to be used in torch.utils.data.DataLoader().
# It pads the texts in each batch such that they have the same sequence length.
def pad_sequence(batch):
    texts  = [text for text, label in batch]
    labels = torch.tensor([label for text, label in batch])
    texts_padded = torch.nn.utils.rnn.pad_sequence(texts,
                                batch_first=True, padding_value = PAD)
    return texts_padded, labels

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True,
                        collate_fn = pad_sequence)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, # Shuffle False for test for consistent evaluation
                        collate_fn = pad_sequence)

"""## Building the encoder-only transformer model for text classification"""

# from transformer_blocks import Encoder
# One can also import Encoder from transformer_blocks.py in my Github repository.
# I copied the code here so that this notebook is self-contained.


class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_embed, dropout=0.0):
        super(MultiHeadedAttention, self).__init__()
        assert d_embed % h == 0 # check the h number
        self.d_k = d_embed//h
        self.d_embed = d_embed
        self.h = h
        self.WQ = nn.Linear(d_embed, d_embed)
        self.WK = nn.Linear(d_embed, d_embed)
        self.WV = nn.Linear(d_embed, d_embed)
        self.linear = nn.Linear(d_embed, d_embed)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x_query, x_key, x_value, mask=None):
        nbatch = x_query.size(0) # get batch size
        # 1) Linear projections to get the multi-head query, key and value tensors
        # x_query, x_key, x_value dimension: nbatch * seq_len * d_embed
        # LHS query, key, value dimensions: nbatch * h * seq_len * d_k
        query = self.WQ(x_query).view(nbatch, -1, self.h, self.d_k).transpose(1,2)
        key   = self.WK(x_key).view(nbatch, -1, self.h, self.d_k).transpose(1,2)
        value = self.WV(x_value).view(nbatch, -1, self.h, self.d_k).transpose(1,2)
        # 2) Attention
        # scores has dimensions: nbatch * h * seq_len * seq_len
        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_k)
        # 3) Mask out padding tokens and future tokens
        if mask is not None:
            scores = scores.masked_fill(mask, float('-inf'))
        # p_atten dimensions: nbatch * h * seq_len * seq_len
        p_atten = torch.nn.functional.softmax(scores, dim=-1)
        p_atten = self.dropout(p_atten)
        # x dimensions: nbatch * h * seq_len * d_k
        x = torch.matmul(p_atten, value)
        # x now has dimensions:nbtach * seq_len * d_embed
        x = x.transpose(1, 2).contiguous().view(nbatch, -1, self.d_embed)
        return self.linear(x) # final linear layer


class ResidualConnection(nn.Module):
  '''residual connection: x + dropout(sublayer(layernorm(x))) '''
  def __init__(self, dim, dropout):
      super().__init__()
      self.drop = nn.Dropout(dropout)
      self.norm = nn.LayerNorm(dim)

  def forward(self, x, sublayer):
      return x + self.drop(sublayer(self.norm(x)))

# I simply let the model learn the positional embeddings in this notebook, since this
# almost produces identital results as using sin/cosin functions embeddings, as claimed
# in the original transformer paper. Note also that in the original paper, they multiplied
# the token embeddings by a factor of sqrt(d_embed), which I do not do here.

class Encoder(nn.Module):
    '''Encoder = token embedding + positional embedding -> a stack of N EncoderBlock -> layer norm'''
    def __init__(self, config):
        super().__init__()
        self.d_embed = config.d_embed
        self.tok_embed = nn.Embedding(config.encoder_vocab_size, config.d_embed)
        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed))
        self.encoder_blocks = nn.ModuleList([EncoderBlock(config) for _ in range(config.N_encoder)])
        self.dropout = nn.Dropout(config.dropout)
        self.norm = nn.LayerNorm(config.d_embed)

    def forward(self, input, mask=None):
        x = self.tok_embed(input)
        x_pos = self.pos_embed[:, :x.size(1), :]
        x = self.dropout(x + x_pos)
        for layer in self.encoder_blocks:
            x = layer(x, mask)
        return self.norm(x)


class EncoderBlock(nn.Module):
    '''EncoderBlock: self-attention -> position-wise fully connected feed-forward layer'''
    def __init__(self, config):
        super(EncoderBlock, self).__init__()
        self.atten = MultiHeadedAttention(config.h, config.d_embed, config.dropout)
        self.feed_forward = nn.Sequential(
            nn.Linear(config.d_embed, config.d_ff),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_ff, config.d_embed)
        )
        self.residual1 = ResidualConnection(config.d_embed, config.dropout)
        self.residual2 = ResidualConnection(config.d_embed, config.dropout)

    def forward(self, x, mask=None):
        # self-attention
        x = self.residual1(x, lambda x: self.atten(x, x, x, mask=mask))
        # position-wise fully connected feed-forward layer
        return self.residual2(x, self.feed_forward)


class Transformer(nn.Module):
    def __init__(self, config, num_classes):
        super().__init__()
        self.encoder = Encoder(config)
        self.linear = nn.Linear(config.d_embed, num_classes)

    def forward(self, x, pad_mask=None):
        x = self.encoder(x, pad_mask)
        return  self.linear(torch.mean(x,-2))

@dataclass
class ModelConfig:
    encoder_vocab_size: int
    d_embed: int
    # d_ff is the dimension of the fully-connected  feed-forward layer
    d_ff: int
    # h is the number of attention head
    h: int
    N_encoder: int
    max_seq_len: int
    dropout: float

def make_model(config):
    model = Transformer(config, num_classes).to(DEVICE)
    # initialize model parameters
    # it seems that this initialization is very important!
    for p in model.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    return model

"""## Train the model with Early Stopping and Checkpointing"""

def train_epoch(model, dataloader):
    model.train()
    losses, acc, count = [], 0, 0
    pbar = tqdm(enumerate(dataloader), total=len(dataloader))
    for idx, (x, y)  in  pbar:
        optimizer.zero_grad()
        features= x.to(DEVICE)
        labels  = y.to(DEVICE)
        pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))
        pred = model(features, pad_mask)

        loss = loss_fn(pred, labels).to(DEVICE)
        loss.backward()
        optimizer.step()

        losses.append(loss.item())
        acc += (pred.argmax(1) == labels).sum().item()
        count += len(labels)
        # report progress
        if idx>0 and idx%50 == 0:
            pbar.set_description(f'train loss={loss.item():.4f}, train_acc={acc/count:.4f}')
    return np.mean(losses), acc/count

def train(model, train_loader, test_loader, epochs, patience=2, checkpoint_dir="checkpoints"):
    best_val_loss = float('inf')
    epochs_no_improve = 0
    checkpoint_files = [] # Keep track of checkpoint files

    os.makedirs(checkpoint_dir, exist_ok=True) # Create checkpoint directory if it doesn't exist

    for ep in range(epochs):
        train_loss, train_acc = train_epoch(model, train_loader)
        val_loss, val_acc = evaluate(model, test_loader)
        print(f'Epoch {ep+1}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}')

        # Checkpointing
        checkpoint_file = os.path.join(checkpoint_dir, f"checkpoint_epoch_{ep+1}.pth")
        torch.save(model.state_dict(), checkpoint_file)
        checkpoint_files.append(checkpoint_file)

        # Keep only last 5 checkpoints
        if len(checkpoint_files) > 5:
            oldest_checkpoint = checkpoint_files.pop(0)
            if os.path.exists(oldest_checkpoint): # Check if file exists before deleting
                os.remove(oldest_checkpoint)

        # Early stopping check
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                print(f"Early stopping triggered after epoch {ep+1} due to no improvement in validation loss for {patience} epochs.")
                break # Exit training loop

def evaluate(model, dataloader): # Corrected evaluate function to use dataloader
    model.eval()
    losses, acc, count = [], 0, 0
    with torch.no_grad():
        for x, y in dataloader:
            features = x.to(DEVICE)
            labels  = y.to(DEVICE)
            pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))
            pred = model(features, pad_mask)
            loss = loss_fn(pred,labels).to(DEVICE)
            losses.append(loss.item())
            acc += (pred.argmax(1) == labels).sum().item()
            count += len(labels)
    return np.mean(losses), acc/count

config = ModelConfig(encoder_vocab_size = vocab_size,
                     d_embed = 32,
                     d_ff = 4*32,
                     h = 1,
                     N_encoder = 1,
                     max_seq_len = max_seq_len,
                     dropout = 0.1
                     )
model = make_model(config)
print(torchinfo.summary(model))
optimizer = torch.optim.Adam(model.parameters())
loss_fn = nn.CrossEntropyLoss()

# Train the model with early stopping and checkpointing
train(model, train_loader, test_loader, epochs=10, patience=2, checkpoint_dir="transformer_checkpoints") # Increased epochs for potential early stopping

"""## Classify new crime info from CSV"""

def classify_new_csv(model, csv_file_path, text_col='crime info', output_csv_path='classified_crimes.csv'):
    """
    Classifies crime information from a new CSV file using the trained transformer model.

    Args:
        model: Trained Transformer model.
        csv_file_path (str): Path to the CSV file containing new crime info.
        text_col (str): Name of the column in the CSV file containing the crime info text.
        output_csv_path (str): Path to save the CSV file with predicted categories.
    """
    new_crime_df = pd.read_csv(csv_file_path)
    predicted_categories = []

    model.eval()  # Set model to evaluation mode
    with torch.no_grad():  # Disable gradient calculations during inference
        for index, row in new_crime_df.iterrows():
            news_text = row[text_col]
            if not isinstance(news_text, str):
                news_text = "" # Handle potential non-string inputs, or skip/handle differently

            x_text = tokenizer(news_text.lower())[0:max_seq_len]
            x_int = torch.tensor([[word_to_id.get(word, UNK) for word in x_text]]).to(DEVICE)
            pad_mask = (x_int == PAD).view(x_int.size(0), 1, 1, x_int.size(-1)) # Create pad mask

            pred_id = model(x_int, pad_mask).argmax(1).item()
            predicted_category = id_to_category[pred_id]
            predicted_categories.append(predicted_category)

    new_crime_df['predicted_category'] = predicted_categories
    new_crime_df.to_csv(output_csv_path, index=False)
    print(f"Predicted categories saved to: {output_csv_path}")

# --- Usage ---
# 1. Ensure your model is trained (run the training part above if needed)
# 2. Specify the path to your new CSV file
new_csv_file = "new_crime_info.csv"  # Replace with the actual path to your new CSV file
output_csv_file = "classified_crime_categories.csv" # Path for the output file

# 3. Call the classification function
classify_new_csv(model, new_csv_file, text_col='crime info', output_csv_path=output_csv_file)

print("Classification complete.")
